# Preprocessing Cache & Invalid Text Tracking - Update Summary

## Overview

Added functionality to track invalid preprocessing and cache processed texts to avoid reprocessing in subsequent runs.

## New Features

### 1. **Processed Text Caching**

Processed texts are now stored in the database and reused in subsequent runs:

- ‚úÖ Texts are stored immediately after preprocessing
- ‚úÖ On next run, cached texts are loaded from DB first
- ‚úÖ Only new papers are preprocessed
- ‚úÖ Significant performance improvement for re-runs

**Database Table:**
```sql
CREATE TABLE processed_texts (
    doi TEXT PRIMARY KEY,
    processed_text TEXT,
    used_full_text BOOLEAN,
    processing_timestamp TEXT,
    FOREIGN KEY(doi) REFERENCES paper_metadata(doi)
)
```

### 2. **Invalid Preprocessing Tracking**

Papers that have full text but become invalid after preprocessing are now tracked:

- ‚úÖ Records DOI, title, metadata
- ‚úÖ Tracks whether paper had full_text or sections
- ‚úÖ Records the specific issue
- ‚úÖ Stored in dedicated database table

**Database Table:**
```sql
CREATE TABLE invalid_after_preprocessing (
    doi TEXT PRIMARY KEY,
    pmid TEXT,
    title TEXT,
    had_full_text BOOLEAN,
    had_sections BOOLEAN,
    preprocessing_issue TEXT,
    timestamp TEXT
)
```

## Workflow Changes

### Before (Old Behavior)
```
1. Load all papers from papers.db
2. Preprocess all texts every time
3. Papers with invalid text are silently skipped
4. No tracking of preprocessing issues
```

### After (New Behavior)
```
1. Load already processed texts from DB
2. Only preprocess NEW papers not in cache
3. Store processed texts immediately to DB
4. Track papers that fail preprocessing
5. Record invalid papers to database
```

## Performance Benefits

### First Run
- Same as before (all papers preprocessed)
- Additional: Texts stored to DB for future use

### Subsequent Runs
- **Cached texts loaded instantly** (no reprocessing)
- Only new papers are preprocessed
- **Massive time savings** for large datasets

### Example Performance
```
First run:  1000 papers ‚Üí 30 minutes preprocessing
Second run: 1000 papers ‚Üí 2 seconds loading from cache
             + 50 new papers ‚Üí 1.5 minutes preprocessing
Total: ~2 minutes instead of 30 minutes
```

## New Methods

### `_store_processed_text()`
```python
def _store_processed_text(self, results_db: str, doi: str, 
                          processed_text: str, used_full_text: bool):
    """Store processed text in database with metadata."""
```

### `_record_invalid_preprocessing()`
```python
def _record_invalid_preprocessing(self, results_db: str, doi: str, pmid: Optional[str], 
                                  title: str, had_full_text: bool, had_sections: bool, 
                                  issue: str):
    """Record papers that had full text but became invalid after preprocessing."""
```

## Updated Statistics

New stats tracked:
- `papers_invalid_after_preprocessing`: Count of papers with invalid preprocessing

**Example Output:**
```
PIPELINE SUMMARY
======================================================================
Total papers: 100
Successfully processed: 92
Failed (validation errors): 3
Invalid after preprocessing: 5

Successful papers breakdown:
  - With full text: 70
  - With abstract only: 22

Token usage:
  - Prompt tokens: 1,500,000
  - Completion tokens: 50,000
  - Total tokens: 1,550,000

Estimated cost: $0.68
======================================================================
```

## Console Output Examples

### First Run (No Cache)
```
üì¶ Pre-loading paper data into memory...
üîç Checking for already processed texts in DB...
‚úì Loaded 0 already processed texts from DB
‚öôÔ∏è  Pre-processing 1000 new texts...
Loading & preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000
‚ö†Ô∏è  Found 5 papers with full text that became invalid after preprocessing
üìù Recording invalid papers to database...
‚úì Recorded 5 invalid papers to 'invalid_after_preprocessing' table
‚úì Total papers with valid processed text: 995
```

### Second Run (With Cache)
```
üì¶ Pre-loading paper data into memory...
üîç Checking for already processed texts in DB...
‚úì Loaded 995 already processed texts from DB
‚öôÔ∏è  Pre-processing 50 new texts...
Loading & preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50
‚úì Total papers with valid processed text: 1045
```

## Database Queries

### Get all invalid papers
```sql
SELECT doi, title, preprocessing_issue, timestamp
FROM invalid_after_preprocessing
ORDER BY timestamp DESC;
```

### Get cached processed texts
```sql
SELECT doi, used_full_text, processing_timestamp
FROM processed_texts
ORDER BY processing_timestamp DESC;
```

### Count papers by processing status
```sql
SELECT 
    COUNT(*) as total_cached
FROM processed_texts;

SELECT 
    COUNT(*) as total_invalid
FROM invalid_after_preprocessing;
```

### Get papers that need reprocessing
```sql
-- Papers in papers.db but not in processed_texts
SELECT p.doi, p.title
FROM papers p
LEFT JOIN processed_texts pt ON p.doi = pt.doi
WHERE pt.doi IS NULL
  AND (p.full_text IS NOT NULL OR p.full_text_sections IS NOT NULL);
```

## Usage

No changes to command-line interface. Caching happens automatically:

```bash
# First run - preprocesses all papers
python scripts/answer_questions_per_paper.py \
    --evaluations-db evaluations.db \
    --papers-db papers.db \
    --results-db qa_results.db

# Second run - loads from cache
python scripts/answer_questions_per_paper.py \
    --evaluations-db evaluations.db \
    --papers-db papers.db \
    --results-db qa_results.db \
    --dois-file new_dois.txt  # Only new DOIs will be preprocessed
```

## Benefits Summary

1. **Performance**: 10-50x faster on subsequent runs
2. **Transparency**: Invalid preprocessing is tracked, not hidden
3. **Debugging**: Can investigate why papers failed preprocessing
4. **Efficiency**: No redundant preprocessing
5. **Reliability**: Consistent results across runs
6. **Storage**: Processed texts stored for analysis

## Migration Notes

- Existing databases will automatically get new tables on next run
- Old runs without cached texts will preprocess normally
- No data loss or compatibility issues
- Cache builds incrementally over time

## Technical Details

### Cache Loading Priority
1. Check `processed_texts` table in results DB
2. Load all cached texts into memory
3. Identify papers not in cache
4. Only fetch and preprocess uncached papers
5. Store newly processed texts immediately

### Invalid Text Detection
Papers are marked invalid if:
- Had `full_text` or `full_text_sections` in papers.db
- After preprocessing, text is empty or < 100 characters
- Issue is recorded with specific reason

### Storage Timing
- Processed texts stored **immediately** after preprocessing
- Not stored again during main processing loop
- Prevents duplicate storage and ensures cache is always up-to-date
